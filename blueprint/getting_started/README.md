# Portworx addon for EKS Blueprint

This  guide will help you install portworx on EKS environment using EKS Blueprints and its kubernetes addon module. In this guide we create a custom IAM policy and attach it to the node groups in EKS cluster to give Portworx the required access. 


The following is a high-level overview of the components generated by this module:

- 1x VPC with private and public subnets, internet gateway etc.
- 1x EKS cluster
- 2x EKS multi-nodes managed node groups
- Installation of Portworx via Helm on the EKS cluster
- Portworx supports native integration with AWS APIs for drive creation and lifecycle management, User can provide drive specification to the Portworx addon using the format below, or instruct Portworx to use already attached volumes 

## Installation

### Prerequisites

Make sure to have the following components installed on your local system:

- [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
- [kubectl](https://kubernetes.io/docs/tasks/tools/)
- [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### Deployment steps

#### Step 1. First, clone the repository:

```shell
git clone https://github.com/portworx/terraform-eksblueprints-portworx-addon.git
```

#### Step 2. Initialise the Terraform module:

```shell
cd blueprints/getting_started
terraform init
```

#### Step 3. Make any necessary adjustments to the `main.tf` file 

Change the variables like region, vpc_cidr, managed_node_groups configurations to set up the cluster according to your requirements. You can additionally supply values to Portworx customise its installation. Refer the configuration section below 

#### Step 4. Export AWS Access key Id and Secret pair as Environment variable 

```shell
export TF_VAR_aws_access_key_id=<access-key-id-value>
export TF_VAR_aws_secret_access_key=<access-key-secret>

```

#### Step 5. Use Terraform to plan a deployment:

```hcl
terraform plan
```

#### Step 6. Review the plan and apply the deployment with Terraform:
Verify the resources created by this execution

```hcl
terraform apply 
```

#### Step 7 Use the AWS CLI to provision a kubeconfig profile for the cluster:

EKS Cluster details can be extracted from "terraform output" command or from AWS Console to get the name of cluster.

```shell
aws eks --region <aws-region> update-kubeconfig --name <cluster-name>
```

#### Step 8. Check that the nodes have created and that Portworx is running:

```shell
kubectl get nodes
```

```shell
kubectl get stc -n kube-system
```

You should see a storage cluster with the name you set running. When its status turns "Online", it means Portworx is successfully up and running.

## Portworx Configuration

The following tables lists the configurable parameters of the Portworx chart and their default values.

| Parameter | Description | Default |
|-----------|-------------| --------|
| `imageVersion` | The image tag to pull | "2.11.0" |
| `useAWSMarketplace` | Set this variable to true if you wish to use AWS marketplace license for Portworx | "false" |
| `clusterName` | Portworx Cluster Name| mycluster |
| `drives` | Semi-colon seperated list of drives to be used for storage. (example: "/dev/sda;/dev/sdb" or "type=gp2,size=200;type=gp3,size=500")  |  "type=gp2,size=200"|
| `useInternalKVDB` | boolen variable to set internal KVDB on/off | true |
| `kvdbDevice` | specify a separate device to store KVDB data, only used when internalKVDB is set to true | type=gp2,size=150 |
| `envVars` | semi-colon-separated list of environment variables that will be exported to portworx. (example: MYENV1=val1;MYENV2=val2) | "" |
| `maxStorageNodesPerZone` | The maximum number of storage nodes desired per zone| 3 |
| `useOpenshiftInstall` | boolen variable to install Portworx on Openshift .| false |
| `etcdEndPoint` | The ETCD endpoint. Should be in the format etcd:http://(your-etcd-endpoint):2379. If there are multiple etcd endpoints they need to be ";" seperated. | "" |
| `dataInterface` | Name of the interface <ethX>.| none |
| `managementInterface` |  Name of the interface <ethX>.| none |
| `useStork` | [Storage Orchestration for Hyperconvergence](https://github.com/libopenstorage/stork).| true  |
| `storkVersion` | Optional: version of Stork. For eg: 2.11.0, when it's empty Portworx operator will pick up version according to Portworx version. | "2.11.0" |
| `customRegistryURL` | URL where to pull Portworx image from | ""  |
| `registrySecret` | Image registery credentials to pull Portworx Images from a secure registry | "" |
| `licenseSecret` | Kubernetes secret name that has Portworx licensing information | ""  |
| `monitoring` | Enable Monitoring on Portworx cluster | false  |
| `enableCSI` | Enable CSI | false  |
| `enableAutopilot` | Enable Autopilot | false  |
| `KVDBauthSecretName` | Refer https://docs.portworx.com/reference/etcd/securing-with-certificates-in-kubernetes to  create a kvdb secret and specify the name of the secret here| none |

## Uninstalling Portworx

1. To uninstall portworx, start by editing the Portworx StorageCluster

```
kubectl edit stc -n kube-system <storage-cluster-name>
```

2. Include the deleteStrategy block in storage cluster specification

```
spec:
    deleteStrategy:
        type:           # Valid values: Uninstall, UninstallAndWipe
```

3. To uninstall, destroy with Terraform - we will use target functionality to destroying in layers which will prevent missed resources or errors.


#### Destroy the add-ons.

```hcl
terraform destroy -target="module.eks_blueprints_kubernetes_addons.module.portworx[0].module.helm_addon"
terraform destroy -target="module.eks_blueprints_kubernetes_addons"
```

#### Destroy the EKS cluster.

```hcl
terraform destroy -target="module.eks_blueprints"
```

#### Destroy the VPC.

```hcl
terraform destroy -target="module.vpc"
```

4. You may also want to login via the AWS console or CLI and manually delete
any remaining EBS snapshots and volumes, they are not deleted as part of the destroy process.




## Modules

| Name | Source | Version |
|------|--------|---------|
| <a name="module_eks_blueprints"></a> [eks\_blueprints](#module\_eks\_blueprints) | github.com/aws-ia/terraform-aws-eks-blueprints | n/a |
| <a name="module_eks_blueprints_kubernetes_addons"></a> [eks\_blueprints\_kubernetes\_addons](#module\_eks\_blueprints\_kubernetes\_addons) | github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons | n/a |
| <a name="module_vpc"></a> [vpc](#module\_vpc) | terraform-aws-modules/vpc/aws | ~> 3.0 |



## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_aws_access_key_id"></a> [aws_access_key_id](#input\_aws\_access\_key\_id) | AWS access key id value| `string` | `""` | yes |
| <a name="input_aws_secret_access_key"></a> [aws_secret_access_key](#input\_aws\_secret\_access\_key) | AWS secret access key value| `string` | `""` | yes |
| <a name="input_cluster_name"></a> [cluster\_name](#input\_cluster\_name) | Name of cluster - used by Terratest for e2e test automation | `string` | `""` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_configure_kubectl"></a> [configure\_kubectl](#output\_configure\_kubectl) | Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig |
| <a name="output_eks_cluster_id"></a> [eks\_cluster\_id](#output\_eks\_cluster\_id) | EKS cluster ID |
| <a name="output_region"></a> [region](#output\_region) | AWS region |
| <a name="output_vpc_cidr"></a> [vpc\_cidr](#output\_vpc\_cidr) | VPC CIDR |
