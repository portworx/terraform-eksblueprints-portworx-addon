# Portworx add-on for EKS Blueprint

This  guide helps you install portworx on EKS environment using EKS Blueprints and its kubernetes add-on module. In this guide, we create a custom IAM policy and attach it to the node groups in EKS cluster to provide Portworx the required access. 


The following list provides an overview of the components generated by this module:

- 1x VPC with private and public subnets, internet gateway, route table, NAT gateway, network interface and network ACL.
- 1x EKS cluster
- 2x EKS multi-nodes managed node groups
- Installation of Portworx via Helm on the EKS cluster
- Portworx supports native integration with AWS APIs for drive creation and lifecycle management. 
- User can provide drive specification to the Portworx add-on using the below format or instruct Portworx to use previously attached volumes 

## Installation

### Prerequisites

Ensure that the following components are installed on your local system:

- [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
- [kubectl](https://kubernetes.io/docs/tasks/tools/)
- [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)


### Deployment steps

#### Step 1. Clone the repository:

```shell
git clone https://github.com/portworx/terraform-eksblueprints-portworx-addon.git
```

#### Step 2. Initialize the Terraform module:

```shell
cd blueprint/portworx_with_iam_policy
terraform init
```

#### Step 3. Make any necessary adjustments to the `main.tf` file 

Change the variables like region, vpc_cidr, managed_node_groups configurations to set up the cluster according to your requirements. You can additionally supply values to Portworx to customize its installation. Refer the configuration section below 

#### Step 4. Apply the deployment with Terraform.
Terraform's target functionality is leveraged to deploy a VPC, an EKS Cluster, and Kubernetes add-ons in separate steps.

Deploy the VPC. This step will take roughly 3 minutes to complete.

```
terraform apply -target="module.vpc"
```
Deploy custom IAM policy.
```
terraform apply -target="aws_iam_policy.portworx_eksblueprint_volumeAccess"
```
Deploy the EKS cluster. This step will take roughly 14 minutes to complete.
```
terraform apply -target="module.eks_blueprints"
```
Deploy the add-ons. This step will take rough 5 minutes to complete.
```
terraform apply -target="module.eks_blueprints_kubernetes_addons"
```


#### Step 5 Use the AWS CLI to provision a kubeconfig profile for the cluster:

EKS Cluster details can be extracted from "terraform output" command or from AWS Console to get the name of cluster.

```shell
aws eks --region <aws-region> update-kubeconfig --name <cluster-name>
```

#### Step 6. Check that the nodes have created and that Portworx is running:

```shell
kubectl get nodes
```

You should see 3 nodes in the list.

```shell
kubectl get stc -n kube-system
```

Result: A storage cluster with set name becomes active which implies Portworx cluster is online.

## Portworx Configuration

The following tables lists the configurable parameters of the Portworx chart and their default values.

| Parameter | Description | Default |
|-----------|-------------| --------|
| `imageVersion` | The image tag to pull | "2.11.0" |
| `useAWSMarketplace` | Set this variable to true if you wish to use AWS marketplace license for Portworx | "false" |
| `clusterName` | Portworx Cluster Name| mycluster |
| `drives` | Semi-colon seperated list of drives to be used for storage. (example: "/dev/sda;/dev/sdb" or "type=gp2,size=200;type=gp3,size=500")  |  "type=gp2,size=200"|
| `useInternalKVDB` | boolen variable to set internal KVDB on/off | true |
| `kvdbDevice` | specify a separate device to store KVDB data, only used when internalKVDB is set to true | type=gp2,size=150 |
| `envVars` | semi-colon-separated list of environment variables that will be exported to portworx. (example: MYENV1=val1;MYENV2=val2) | "" |
| `maxStorageNodesPerZone` | The maximum number of storage nodes desired per zone| 3 |
| `useOpenshiftInstall` | boolen variable to install Portworx on Openshift .| false |
| `etcdEndPoint` | The ETCD endpoint. Should be in the format etcd:http://(your-etcd-endpoint):2379. If there are multiple etcd endpoints they need to be ";" seperated. | "" |
| `dataInterface` | Name of the interface <ethX>.| none |
| `managementInterface` |  Name of the interface <ethX>.| none |
| `useStork` | [Storage Orchestration for Hyperconvergence](https://github.com/libopenstorage/stork).| true  |
| `storkVersion` | Optional: version of Stork. For eg: 2.11.0, when it's empty Portworx operator will pick up version according to Portworx version. | "2.11.0" |
| `customRegistryURL` | URL where to pull Portworx image from | ""  |
| `registrySecret` | Image registery credentials to pull Portworx Images from a secure registry | "" |
| `licenseSecret` | Kubernetes secret name that has Portworx licensing information | ""  |
| `monitoring` | Enable Monitoring on Portworx cluster | false  |
| `enableCSI` | Enable CSI | false  |
| `enableAutopilot` | Enable Autopilot | false  |
| `KVDBauthSecretName` | Refer https://docs.portworx.com/reference/etcd/securing-with-certificates-in-kubernetes to  create a kvdb secret and specify the name of the secret here| none |
| `deleteType` | Specify which strategy to use while Uninstalling Portworx. "Uninstall" values only removes Portworx but with "UninstallAndWipe" value all data from your disks including the Portworx metadata is also wiped permanently | UninstallAndWipe |


## Uninstalling Portworx:

This section describes how to uninstall Portworx and remove its Kubernetes specs. When uninstalling, you may choose to either keep the the data on your drives, or wipe them completely.

1. Start by choosing between one of two deleteStrategy option and updating the terraform script.

```
portworx_chart_values = { 
    deleteType =    # Valid values: "Uninstall" and "UninstallAndWipe"
    # other custom values
}
```

```Uninstall``` only removes Portworx but ```UninstallAndWipe``` also remove all data from your disks permanently including the Portworx metadata. Use caution when applying the DeleteStrategy spec. Default value is set to ```UninstallAndWipe```.

2. Perform a Terraform apply to apply the change

```
terraform apply -target="module.eks_blueprints_kubernetes_addons"
```


3. Perform Terraform destroy using it target functionality to destroy in layers which prevents missed resources or errors.


#### Destroy the add-ons.

```hcl
terraform destroy -target="module.eks_blueprints_kubernetes_addons.module.portworx[0].module.helm_addon"
terraform destroy -target="module.eks_blueprints_kubernetes_addons"
```

#### Destroy the EKS cluster.

```hcl
terraform destroy -target="module.eks_blueprints"
```
#### Destroy the IAM policy.

```hcl
terraform destroy -target="aws_iam_policy.portworx_eksblueprint_volumeAccess"
```

#### Destroy the VPC.

```hcl
terraform destroy -target="module.vpc"
```

4. You may also want to login via the AWS console or CLI and manually delete
any remaining EBS snapshots and volumes, they are not deleted as part of the destroy process.




## Modules

| Name | Source | Version |
|------|--------|---------|
| <a name="module_eks_blueprints"></a> [eks\_blueprints](#module\_eks\_blueprints) | github.com/aws-ia/terraform-aws-eks-blueprints | n/a |
| <a name="module_eks_blueprints_kubernetes_addons"></a> [eks\_blueprints\_kubernetes\_addons](#module\_eks\_blueprints\_kubernetes\_addons) | github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons | n/a |
| <a name="module_vpc"></a> [vpc](#module\_vpc) | terraform-aws-modules/vpc/aws | ~> 3.0 |



## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_cluster_name"></a> [cluster\_name](#input\_cluster\_name) | Name of cluster - used by Terratest for e2e test automation | `string` | `""` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_configure_kubectl"></a> [configure\_kubectl](#output\_configure\_kubectl) | Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig |
| <a name="output_eks_cluster_id"></a> [eks\_cluster\_id](#output\_eks\_cluster\_id) | EKS cluster ID |
| <a name="output_region"></a> [region](#output\_region) | AWS region |
| <a name="output_vpc_cidr"></a> [vpc\_cidr](#output\_vpc\_cidr) | VPC CIDR |
