# Portworx addon for EKS Blueprint

This  guide will help you install portworx on EKS environment using EKS Blueprints and its kubernetes addon module. In this guide we create a custom IAM policy and attach it to the node groups in EKS cluster to give Portworx the required access. 


The following is a high-level overview of the components generated by this module:

- 1x VPC with private and public subnets, internet gateway etc.
- 1x EKS cluster
- 1x IAM policy with custom permission for Portworx
- 1x EKS multi-node managed node group with the above IAM policy attached
- Installation of Portworx via Helm on the EKS cluster
- Portworx supports native integration with AWS APIs for drive creation and lifecycle management, User can provide drive specification to the Portworx addon using the format below, or instruct Portworx to use already attached volumes 

## Installation

### Prerequisites

Make sure to have the following components installed on your local system:

- [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
- [kubectl](https://kubernetes.io/docs/tasks/tools/)
- [terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)

### Deployment steps

#### Step 1. First, clone the repository:

```shell
git clone https://github.com/portworx/terraform-eksblueprints-portworx-addon.git
```

#### Step 2. Initialise the Terraform module:

```shell
cd blueprints/portworx_with_iam_policy
terraform init
```

#### Step 3. Make any necessary adjustments to the `main.tf` file 

Change the variables like region, vpc_cidr, managed_node_groups configurations to set up the cluster according to your requirements. You can additionally supply values to Portworx customise its installation. Refer the configuration section below 


#### Step 4. Use Terraform to plan a deployment:

```hcl
terraform plan
```

#### Step 5. Review the plan and apply the deployment with Terraform.
We will leverage Terraform's target functionality to deploy a VPC, an EKS Cluster, and Kubernetes add-ons in separate steps.

Deploy the VPC. This step will take roughly 3 minutes to complete.

```
terraform apply -target="module.vpc"
```
Deploy custom IAM policy.
```
terraform apply -target="aws_iam_policy.portworx_eksblueprint_policy"
```
Deploy the EKS cluster. This step will take roughly 14 minutes to complete.
```
terraform apply -target="module.eks_blueprints"
```
Deploy the add-ons. This step will take rough 5 minutes to complete.
```
terraform apply -target="module.eks_blueprints_kubernetes_addons"
```



#### Step 6 Use the AWS CLI to provision a kubeconfig profile for the cluster:

EKS Cluster details can be extracted from "terraform output" command or from AWS Console to get the name of cluster.

```shell
aws eks --region <aws-region> update-kubeconfig --name <cluster-name>
```

#### Step 7. Check that the nodes have created and that Portworx is running:

```shell
kubectl get nodes
```

You should see 3 nodes in the list.

```shell
kubectl get stc -n kube-system
```

You should see a storage cluster with the name you set running. When its status turns "Online", it means Portworx is successfully up and running.

## Portworx Configuration

The following tables lists the configurable parameters of the Portworx chart and their default values.

| Parameter | Description | Default |
|-----------|-------------| --------|
| `imageVersion` | The image tag to pull | "2.11.0" |
| `clusterName` | Portworx Cluster Name| mycluster |
| `drives` | Semi-colon seperated list of drives to be used for storage. (example: "/dev/sda;/dev/sdb" or "type=gp2,size=200;type=gp3,size=500")  |  "type=gp2,size=200"|
| `useInternalKVDB` | boolen variable to set internal KVDB on/off | true |
| `kvdbDevice` | specify a separate device to store KVDB data, only used when internalKVDB is set to true | type=gp2,size=150 |
| `envVars` | semi-colon-separated list of environment variables that will be exported to portworx. (example: MYENV1=val1;MYENV2=val2) | "" |
| `maxStorageNodesPerZone` | The maximum number of storage nodes desired per zone| 3 |
| `useOpenshiftInstall` | boolen variable to install Portworx on Openshift .| false |
| `etcdEndPoint` | The ETCD endpoint. Should be in the format etcd:http://(your-etcd-endpoint):2379. If there are multiple etcd endpoints they need to be ";" seperated. | "" |
| `dataInterface` | Name of the interface <ethX>.| none |
| `managementInterface` |  Name of the interface <ethX>.| none |
| `useStork` | [Storage Orchestration for Hyperconvergence](https://github.com/libopenstorage/stork).| true  |
| `storkVersion` | Optional: version of Stork. For eg: 2.11.0, when it's empty Portworx operator will pick up version according to Portworx version. | "2.11.0" |
| `customRegistryURL` | URL where to pull Portworx image from | ""  |
| `registrySecret` | Image registery credentials to pull Portworx Images from a secure registry | "" |
| `licenseSecret` | Kubernetes secret name that has Portworx licensing information | ""  |
| `monitoring` | Enable Monitoring on Portworx cluster | false  |
| `enableCSI` | Enable CSI | false  |
| `enableAutopilot` | Enable Autopilot | false  |
| `KVDBauthSecretName` | Refer https://docs.portworx.com/reference/etcd/securing-with-certificates-in-kubernetes to  create a kvdb secret and specify the name of the secret here| none |

## Uninstalling Portworx

1. To uninstall portworx, start by editing the Portworx StorageCluster

```
kubectl edit stc -n kube-system <storage-cluster-name>
```

2. Include the deleteStrategy block in storage cluster specification

```
spec:
    deleteStrategy:
        type:           # Valid values: Uninstall, UninstallAndWipe
```

3. To uninstall, destroy with Terraform - we will use target functionality to destroying in layers which will prevent missed resources or errors.


#### Destroy the add-ons.

```hcl
terraform destroy -target="module.eks_blueprints_kubernetes_addons.module.portworx[0].module.helm_addon"
terraform destroy -target="module.eks_blueprints_kubernetes_addons"
```

#### Destroy the EKS cluster.

```hcl
terraform destroy -target="module.eks_blueprints"
```
#### Destroy the IAM policy.

```hcl
terraform destroy -target="aws_iam_policy.portworx_eksblueprint_policy"
```

#### Destroy the VPC.

```hcl
terraform destroy -target="module.vpc"
```

4. You may also want to login via the AWS console or CLI and manually delete
any remaining EBS snapshots and volumes, they are not deleted as part of the destroy process.




## Modules

| Name | Source | Version |
|------|--------|---------|
| <a name="module_eks_blueprints"></a> [eks\_blueprints](#module\_eks\_blueprints) | github.com/aws-ia/terraform-aws-eks-blueprints | n/a |
| <a name="module_eks_blueprints_kubernetes_addons"></a> [eks\_blueprints\_kubernetes\_addons](#module\_eks\_blueprints\_kubernetes\_addons) | github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons | n/a |
| <a name="module_vpc"></a> [vpc](#module\_vpc) | terraform-aws-modules/vpc/aws | ~> 3.0 |



## Inputs

| Name | Description | Type | Default | Required |
|------|-------------|------|---------|:--------:|
| <a name="input_cluster_name"></a> [cluster\_name](#input\_cluster\_name) | Name of cluster - used by Terratest for e2e test automation | `string` | `""` | no |

## Outputs

| Name | Description |
|------|-------------|
| <a name="output_configure_kubectl"></a> [configure\_kubectl](#output\_configure\_kubectl) | Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig |
| <a name="output_eks_cluster_id"></a> [eks\_cluster\_id](#output\_eks\_cluster\_id) | EKS cluster ID |
| <a name="output_region"></a> [region](#output\_region) | AWS region |
| <a name="output_vpc_cidr"></a> [vpc\_cidr](#output\_vpc\_cidr) | VPC CIDR |
